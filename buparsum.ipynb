{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selinkarakus/BUParsum/blob/main/buparsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUBinW0DslhB",
        "outputId": "e4aa9fb7-ae01-40f1-d817-2ab27459f5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " [1] English \n",
            " [2] German \n",
            " [3] Turkish \n",
            " [4] French \n",
            " [5] Spanish \n",
            " [6] I will provide my own English text. \n",
            "Enter your options or options with commas in between: \n",
            " (e.g.: \"1\" or \"1, 2\") \n",
            "1, 2\n",
            "Please enter the feature or features you would like to list with commas in between each feature:\n",
            "(ex.: \"DET\" or \"ADV, SUB\", etc.)\n",
            "DET, NOUN\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-96de6c641403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m                   \u001b[0mf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i}\\t{j}\\t{idt}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m   \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_lang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parse_datatest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mparse_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-96de6c641403>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(file_names, output_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m           \u001b[0mud_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{file_name}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0mREGEX_BLOCK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(\\# sent_id = (.+?)\\n# text = (.+?)\\n)((.|\\n)+?)\\n(?=# sent_id)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/en_partut-ud-dev.conllu'"
          ]
        }
      ],
      "source": [
        "import enum\n",
        "import re\n",
        "import sys\n",
        "from collections import defaultdict \n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\" [1] English \\n [2] German \\n [3] Turkish \\n [4] French \\n [5] Spanish \\n [6] I will provide my own English text. \")\n",
        "options = []\n",
        "options = input (\"Enter your options or options with commas in between: \\n (e.g.: \\\"1\\\" or \\\"1, 2\\\") \\n\")\n",
        "options = options.split(\", \")\n",
        "\n",
        "lang_dict = {1: 'English', 2:'German', 3:'Turkish', 4:'French', 5: 'Spanish'}\n",
        "\n",
        "orig_input = input(\"Please enter the feature or features you would like to list with commas in between each feature:\\n(ex.: \\\"DET\\\" or \\\"ADV, SUB\\\", etc.)\\n\")\n",
        "temp_inp = orig_input.split(', ')\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for option in options:\n",
        "  input_list = []\n",
        "  if option == '1':\n",
        "    default_lang = '/content/en_partut-ud-dev.conllu'\n",
        "    color = '#F51B1B'\n",
        "    language = 'English'\n",
        "  elif option == '2':\n",
        "    default_lang = '/content/de_gsd-ud-dev.conllu'\n",
        "    color = '#BB1BF5'\n",
        "    language = 'German'\n",
        "  elif option == '3':\n",
        "    default_lang = '/content/tr_boun-ud-dev.conllu'  \n",
        "    color = '#1B5CF5'\n",
        "    language = 'Turkish'\n",
        "  elif option == '4':\n",
        "    default_lang = '/content/fr_partut-ud-dev.conllu'\n",
        "    color = '#1BF5D8'\n",
        "    language = 'French'\n",
        "  elif option == '5':\n",
        "    default_lang = '/content/es_gsd-ud-dev.conllu'\n",
        "    color = '#F5961B'\n",
        "    language = 'Spanish'\n",
        "  elif option == '6':\n",
        "    default_lang = []\n",
        "    color = '#C9F51B'\n",
        "    language = 'Your data'\n",
        "  else:\n",
        "    print(\"Invalid input.\")\n",
        "    #sys.exit()\n",
        "\n",
        "  main_output = []\n",
        "\n",
        "\n",
        "  def parser(file_names, output_name):\n",
        "  #Some parts of this code was taken from a project by Karahan Sahin\n",
        "  #https://github.com/karahan-sahin/Char-Level-Morphological-Parsing-with-Transformers\n",
        "\n",
        "      output = []\n",
        "\n",
        "      for file_name in file_names:\n",
        "\n",
        "          ud_file = open(f'{file_name}', 'r', encoding='utf-8').read()\n",
        "\n",
        "          REGEX_BLOCK = '(\\# sent_id = (.+?)\\n# text = (.+?)\\n)((.|\\n)+?)\\n(?=# sent_id)'\n",
        "\n",
        "          parses = re.findall(REGEX_BLOCK, ud_file)\n",
        "\n",
        "          pos_unique = set()\n",
        "\n",
        "          for parse in parses:\n",
        "\n",
        "              idt = parse[1]\n",
        "              lines = parse[3].split('\\n')\n",
        "\n",
        "              sent = \"\"\n",
        "              forms = []\n",
        "\n",
        "              for line in lines[:-1]:\n",
        "\n",
        "                  tokens = line.split('\\t')\n",
        "\n",
        "                  form = tokens[1]\n",
        "                  lemma = tokens[2]\n",
        "                  POS = tokens[3] \n",
        "                  features = tokens[5]\n",
        "\n",
        "                  pos_unique.add(POS)\n",
        "\n",
        "                  #print(line)\n",
        "\n",
        "                  if lemma == \"_\" and len(forms) > 0:\n",
        "                      forms[-1] = forms[-1] + form\n",
        "                  \n",
        "                  else:\n",
        "                      forms.append(form) \n",
        "\n",
        "                  feature_find = '+'.join([i for i in features.split('|')])\n",
        "\n",
        "                  features = '+'.join([i for i in features.split('|')])\n",
        "                  \n",
        "                  if lemma == \"_\":\n",
        "\n",
        "                      sent += f\"DB^{POS}+{features}\"\n",
        "\n",
        "                  else:\n",
        "\n",
        "                      if POS == \"PUNCT\":\n",
        "\n",
        "                            sent += f\" {form}+{POS}\"\n",
        "\n",
        "                      else:\n",
        "\n",
        "                          if features:\n",
        "\n",
        "                              sent += f\" {lemma}+{POS}+{features}\"\n",
        "\n",
        "                          else:\n",
        "\n",
        "                              sent += f\" {lemma}+{POS}\"\n",
        "\n",
        "              output.append((forms, sent.lstrip().split(), idt))\n",
        "\n",
        "      with open(f\"{output_name}.txt\", \"w+\", encoding='utf-8') as f_out:\n",
        "\n",
        "          for f, t, idt in output:\n",
        "\n",
        "              for i, j in zip(f, t):\n",
        "\n",
        "                  f_out.write(f\"{i}\\t{j}\\t{idt}\\n\")\n",
        "\n",
        "  parser([default_lang], \"parse_datatest\")\n",
        "\n",
        "  def parse_sentences(file_names, output_name):\n",
        "\n",
        "      output = []\n",
        "\n",
        "      for file_name in file_names:\n",
        "\n",
        "          print(file_name)\n",
        "\n",
        "          ud_file = open(f'{file_name}', 'r', encoding='utf-8').read()\n",
        "\n",
        "          REGEX_BLOCK = '(\\# sent_id = (.+?)\\n# text = (.+?)\\n)((.|\\n)+?)\\n(?=# sent_id)'\n",
        "\n",
        "          parses = re.findall(REGEX_BLOCK, ud_file)\n",
        "\n",
        "          for parse in parses:\n",
        "\n",
        "              output.append((parse[2], parse[1]))\n",
        "\n",
        "      with open(f\"{output_name}\", \"w+\", encoding='utf-8') as f_out:\n",
        "\n",
        "          for sent, idt in output:\n",
        "\n",
        "            f_out.write(f\"{sent}\\t{idt}\\n\")\n",
        "\n",
        "  with open('/content/parse_datatest.txt', 'r') as file:\n",
        "      data = file.read().replace('\\n','')\n",
        "  \n",
        "  for i in range(len(temp_inp)):\n",
        "    input_list.append(temp_inp[i])\n",
        "\n",
        "  output_dict = {}\n",
        "  for element in input_list:\n",
        "    output_dict[element] = len(re.findall(f\".+?{element}\", data))\n",
        "  output_dict\n",
        "  temp_df = pd.DataFrame({'x':list(output_dict.keys()),'y': list(output_dict.values())})\n",
        "  temp_df['Language'] = language\n",
        "  df=pd.concat([df, temp_df])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.barplot(x='x',y='y',data=df ,hue='Language')\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "languages = list(plot_dict.keys())\n",
        "X = list(output_dict.keys())\n",
        "X_axis = np.arange(len(X))\n",
        "plt.figure(figsize=(5,8))\n",
        "for i in range(len(languages)):\n",
        "  rect = plt.bar(X_axis - 0.2+0.4*i,list(plot_dict[languages[i]].values()), 0.8/len(languages) , label=languages[i])\n",
        "\n",
        "plt.legend()\n",
        "plt.xticks(X_axis, X)\n",
        "plt.show()\n",
        "\n",
        "  plt.figure(figsize=(5,8))\n",
        "  bar1 = plt.bar(list(output_dict.keys()), list(output_dict.values()), color = f'{color}' )\n",
        "  for rect in bar1:\n",
        "      plt.title(f\"{language}\")\n",
        "      height = rect.get_height()\n",
        "      plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.0f}', ha='center', va='bottom')\n",
        "  plt.show()\n",
        "'''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5NkNrCYdb4Z",
        "outputId": "48c716fb-268a-416f-f4db-32f1f0472b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADV': 105, 'Definite': 197, 'NOUN': 561}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(plot_dict[languages[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQJVWP2hddQE",
        "outputId": "5d71cde9-43cb-4642-cf02-630b5f7d5ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Copy of buparsum.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}